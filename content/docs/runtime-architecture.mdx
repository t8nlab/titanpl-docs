---
title: "Gravity Runtime: Multi Threaded, Strictly Synchronous V8"
description: "Understanding Gravity - TitanPL's strictly synchronous V8 runtime with multi-threaded execution."
---

import { Callout } from "fumadocs-ui/components/callout"

## ðŸŒŒ Introducing Gravity

**Gravity** is the core force that holds the entire TitanPL system together. 

It's TitanPL's strictly synchronous V8 runtime engine â€” a fundamental departure from traditional JavaScript runtimes. While Node.js, Bun, and Deno rely on event loops for concurrency, Gravity achieves massive parallelism through **native multi-threading** and **synchronous execution**.

Think of Gravity as the gravitational force in the Titan ecosystem:
- It **binds** your JavaScript code to native Rust performance
- It **grounds** execution in predictable, deterministic patterns
- It **pulls** everything together with zero lock contention

---

## ðŸ§± What TitanPL Is Not

TitanPL is **not a Node.js framework**. It is fundamentally different.

TitanPL is a **Rust server with embedded V8 engines** that executes JavaScript/TypeScript **synchronously** in isolated worker threads.

<Callout type="info">
  Unlike Node.js, Bun, or Deno, **Gravity** does not run an event loop. Code executes synchronously from request entry to response exit â€” making execution deterministic, predictable, and exceptionally fast for compute-bound workloads.
</Callout>

---

## ðŸŽ¯ Key Architectural Principles

### 1. No Event Loop in Workers

Unlike Node.js which uses libuv's event loop, or Deno/Bun which use Tokio's async runtime, **TitanPL workers execute synchronously**.

- **No event queue**
- **No microtask queue**
- **No async callbacks**
- **No `setTimeout`, `setInterval`, or `Promise` chains**

### 2. Request-Driven Execution

Each worker processes one request at a time:

1. Worker receives request from Rust dispatcher
2. Worker blocks and executes the action synchronously
3. Worker returns response
4. Worker awaits next request

**No concurrency within a single worker.** Scaling happens by adding more workers, not through async I/O.

### 3. Blocking I/O

All I/O operations block the worker thread:

```javascript
// âŒ This will NOT work (no async/await support)
export const fetchUser = defineAction(async (req) => {
  const response = await t.fetch("https://api.example.com/user");
  return response;
});

// âœ… Use synchronous blocking calls instead
export const fetchUser = defineAction((req) => {
  // This blocks the worker until the HTTP response arrives
  const response = t.fetch("https://api.example.com/user");
  return response;
});
```

The Rust runtime handles the async I/O under the hood, but to JavaScript, it appears completely synchronous.

### 4. Deterministic Execution

All code runs **linearly**, top to bottom:

- Easier debugging (no callback hell)
- Predictable stack traces
- No race conditions within a single request
- Reproducible behavior

### 5. True Isolation

Each worker owns an **independent V8 isolate** with:

- Zero shared state
- No cross-worker communication
- Isolated heap and garbage collection
- Crash isolation (one worker crash doesn't affect others)

### 6. No `require` or `import.meta`

- **ES6 imports only** (`import`/`export`)
- Dependencies are bundled with esbuild
- No dynamic `require()` at runtime
- No Node.js module resolution

### 7. No Async/Await

JavaScript actions **cannot use**:

- `async`/`await`
- `Promise` chains
- `setTimeout` / `setInterval`
- `process.nextTick`
- Any other asynchronous primitives

---

## ðŸ”„ Synchronous Execution Model

Here's how a request flows through TitanPL:

![Synchronous Execution Flow](/synchronous_execution_flow.png)

### Step-by-Step Breakdown

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Incoming HTTP Request                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Axum HTTP Server     â”‚
         â”‚    (Rust, async)       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â”‚ Dispatch to Worker
                  â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚      Worker Thread (Rust)        â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
    â”‚  â”‚   V8 Isolate               â”‚  â”‚
    â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
    â”‚  â”‚  â”‚  Execute Action      â”‚  â”‚  â”‚ â—„â”€â”€ Synchronous, blocking
    â”‚  â”‚  â”‚  (JavaScript/TS)     â”‚  â”‚  â”‚
    â”‚  â”‚  â”‚                      â”‚  â”‚  â”‚
    â”‚  â”‚  â”‚  t.fetch() â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”¼â”€â”€â”¼â”€â”€â–º Blocks until HTTP response
    â”‚  â”‚  â”‚         â”‚            â”‚  â”‚  â”‚
    â”‚  â”‚  â”‚         â–¼            â”‚  â”‚  â”‚
    â”‚  â”‚  â”‚  return { ... }      â”‚  â”‚  â”‚
    â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â”‚ Return response
               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   HTTP Response Sent    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Points:**

- The Rust Axum server is async (for network I/O efficiency)
- Each worker executes JavaScript **synchronously**
- Blocking calls (like `t.fetch()`) pause the worker until complete
- The worker returns a response and becomes available for the next request

---

## ðŸš€ Multi-Threaded Architecture

TitanPL achieves massive concurrency through **native multi-threading**, not async I/O.

![Multi-Threaded Architecture](/multi_threaded_architecture.png)

### How It Works

TitanPL spins up a **worker pool**, where each worker owns:

- **Its own V8 isolate** (completely independent JavaScript runtime)
- **Its own context** (global scope, compiled actions)
- **Its own compiled actions** (pre-compiled bytecode)
- **No event loop** (synchronous execution only)

Workers **never share locks**, **never block each other**, and **never wait for global state**.

### Concurrency Model

```
HTTP Requests â†’ Rust Load Balancer â†’ Workers (parallel execution)
                                      â”œâ”€ Worker 1 (V8 Isolate, Context, Actions)
                                      â”œâ”€ Worker 2 (V8 Isolate, Context, Actions)
                                      â”œâ”€ Worker 3 (V8 Isolate, Context, Actions)
                                      â””â”€ Worker N (V8 Isolate, Context, Actions)
```

**Each CPU core runs JavaScript independently:**

- Zero lock contention
- Linear scaling with core count
- Massive throughput under real traffic
- No "Stop-the-World" garbage collection across workers

---

## ðŸ†š TitanPL vs. Traditional JavaScript Runtimes

![TitanPL vs Node.js](/titanpl_vs_nodejs.png)

| Feature | Traditional JS Runtimes | TitanPL |
| :--- | :--- | :--- |
| **Execution Model** | Event Loop (libuv/Tokio) | Synchronous Workers |
| **Concurrency** | Async I/O (Single Main Thread) | Native Multi-threading |
| **Scaling** | Vertical (limited by event loop) | Linear (scales with cores) |
| **Memory** | Shared Heap (GC blocks all) | Isolated Heap per Worker |
| **CPU Utilization** | Single-core bottleneck | Full Hardware Utilization |
| **Async/Await** | âœ… Required for I/O | âŒ Not Supported |
| **Promises** | âœ… Yes | âŒ No |
| **Event Loop** | âœ… Yes | âŒ No |

### Traditional Runtimes (Node.js, Bun, Deno)

**Strengths:**
- Excellent for I/O-heavy workloads
- Non-blocking async operations
- Rich ecosystem of async libraries

**Limitations:**
- Single-threaded execution for user code
- Event loop can become bottleneck
- Async overhead for CPU-bound tasks

### TitanPL

**Strengths:**
- True multi-threaded JavaScript execution
- Linear scaling with CPU cores
- Deterministic, predictable execution
- No async overhead

**Limitations:**
- Cannot use async/await or Promises
- Not ideal for I/O-heavy services with high concurrency
- Requires more workers to scale (trades memory for performance)

---

## âš¡ When to Use TitanPL

### âœ… Perfect For

- **CPU-bound or compute-heavy services**
  - AI/ML inference
  - Data transformations
  - Complex business logic
  - Cryptography

- **Deterministic execution requirements**
  - Financial calculations
  - Gaming backends with tick-based logic
  - Reproducible computations

- **Linear debugging workflows**
  - Simple stack traces
  - No callback hell
  - Predictable execution order

- **Predictable memory usage per worker**
  - Isolated heaps
  - No shared state management

- **Crash isolation**
  - One worker crash doesn't affect others
  - Easy recovery and error handling

### âŒ Not Ideal For

- **I/O-heavy services with high concurrency**
  - Use Node.js, Deno, or Bun instead
  - Async I/O is more efficient for these workloads

- **Applications requiring `setTimeout`, Promises, or async/await**
  - TitanPL does not support async primitives

- **Real-time event-driven architectures**
  - Event loops are better suited for this

---

## ðŸ”§ Migration from Async Patterns

If you're coming from Node.js, **do not** try to use async patterns.

### âŒ This Will NOT Work

```javascript
export const processData = defineAction(async (req) => {
  const data = await fetchFromDatabase();
  const result = await processWithAI(data);
  return { result };
});
```

### âœ… Use Synchronous Blocking Calls

```javascript
export const processData = defineAction((req) => {
  // These calls block until complete
  const data = t.db.query("SELECT * FROM users");
  const result = processWithAI(data); // Synchronous function
  return { result };
});
```

### Chaining Operations

Instead of Promise chains:

```javascript
// âŒ Node.js style
const result = await fetch(url1)
  .then(res => res.json())
  .then(data => fetch(url2, { body: data }))
  .then(res => res.json());

// âœ… TitanPL style
const res1 = t.fetch(url1);
const data = JSON.parse(res1.body);
const res2 = t.fetch(url2, { body: JSON.stringify(data) });
const result = JSON.parse(res2.body);
```

---

## ðŸŽ¯ Why Multi-Threading Matters

Traditional JavaScript runtimes:

- Run user code on **one thread**
- Rely heavily on **async I/O** to "fake" concurrency
- Collapse under CPU-heavy workloads

**TitanPL eliminates this limitation:**

### Every worker can execute CPU-bound JavaScript simultaneously â€” **zero blocking**.

This is ideal for:

- ðŸ¤– **AI Systems** â€” Parallel processing of heavy logic and data
- ðŸŽ® **Gaming Backends** â€” Low-latency, real-time state synchronization
- ðŸ“ˆ **Real-time Analytics** â€” High-frequency data transformations
- âš¡ **Compute-heavy Actions** â€” Multi-user concurrency without blocking

---

## ðŸŒŒ TitanPL: The Future of JavaScript Backend Engines

With native Rust + V8 multi-threading, TitanPL becomes:

- **Faster** for CPU-bound workloads
- **More scalable** under load
- **Safer and more predictable**
- **Architecturally modern**
- **Ready for enterprise-grade traffic**

<Callout type="warn">
  **Remember:** Gravity trades async flexibility for synchronous predictability and true multi-threaded performance. Choose the right tool for your workload.
</Callout>

